# 🏗️ Werfen Data Impact Case - End-to-End Enterprise Data Architecture

## 📋 Executive Summary

This project demonstrates a **modern and scalable data architecture** specifically designed for Werfen, implementing an end-to-end pipeline with industry best practices and enterprise-grade components.

## 📄 Business Case Presentation

For a detailed overview of the business case, strategic framework, and expected impact, please see the full presentation deck:

**📄 [View the Business Case Presentation PDF](2025-07-02_Werfen_BusinessCase_DanielMoreno.pdf)**

## 🎯 Primary Objectives

- Demonstrate business vision and focus on impact
- Demonstrate technical expertise in modern data architectures
- Implement ML pipeline for customer segmentation (Business Personas)

## ♟️ Strategy: Our "Impact-First" Approach

<details>
<summary>Click to see the strategic framework that guided this project</summary>

Before writing any code, we define our plan. Our goal is not just to analyze data, but to solve a real business problem. To ensure our technical efforts are always aligned with business value, we follow a philosophy we call "Impact-First".

This is a top-down framework that starts with a high-level business goal and translates it into a concrete data solution. Let's apply it to this case.

### The Strategic Framework: From Goal to Action
Here is how we break down the problem methodically:

1.  **The GOAL (KGI): Increase Net Revenue +10%**  
    This is our ultimate objective. It's the "North Star" that guides every decision.

2.  **The COMPASS (KPIs): Our Key Levers**  
    To achieve our goal, we need a compass. We will focus on moving three key levers (KPIs) that directly influence revenue:
    - ↑ Customer Retention: Keep more of our valuable customers.
    - ↑ Average Revenue per User (ARPU): Increase the value generated by each customer.
    - ↑ % of "Champion" Customers: Convert more customers into our most profitable segment.

3.  **The SITUATION: The "One-Size-Fits-All" Challenge**  
    The primary obstacle to moving these KPIs is our current "one-size-fits-all" approach to customers. This leads to three major problems:
    - We Don't Know: Who our loyal customers are versus those at-risk.
    - We Can't: Execute effective, targeted sales campaigns for up-selling or cross-selling.
    - We Don't Learn: From the patterns of our top performers to create winning, repeatable playbooks.

4.  **The STRATEGY: Actionable Customer Segmentation**  
    The most effective strategy to solve this is to implement data-driven customer segmentation. By grouping customers into clear archetypes based on their real behavior, we can finally know who they are, be able to take targeted action, and learn from the results.

5.  **The TACTICS: How We'll Use the Segments**  
    This strategy enables specific, high-impact tactics:
    - Grow: Identify high-potential clusters for targeted campaigns.
    - Optimize: Reduce costs on low-potential segments.
    - Learn: Replicate the success of our "Champion" customers.

6.  **The DATA PRODUCTS: The Tools We Need to Build**  
    To execute these tactics, our framework tells us exactly what to build:
    - An ML Model to generate the customer personas automatically.
    - Core Datasets (Fact Tables) to track our KPIs and measure campaign results.
    - A Tactical Dashboard to empower the Sales team with daily, actionable insights.

### The Key Metrics We Will Build (Our Features)
Now that we have a clear plan, we can define the specific metrics needed to power our ML model and data products. We will transform raw transaction data into these meaningful behavioral features for each customer:

| Metric (Feature) | Simple Description | What does it tell us about the customer? | How is it calculated? (per customer) |
| :--- | :--- | :--- | :--- |
| **total_sold_quantity** | Total volume the customer purchased. | Their overall purchasing power and value. | `SUM(sold_quantity)` |
| **total_foc_quantity** | Total volume of free items received. | Their dependency on free promotions. | `SUM(foc_quantity)` |
| **foc_ratio** | The proportion of free items out of their total items. | **The "Loyalty Thermometer."** A high ratio may signal a promotion-driven customer, not a truly loyal one. | `total_foc_quantity / (total_sold_quantity + total_foc_quantity)` |
| **total_sold_transactions** | The number of times they made a purchase. | Their level of engagement and purchase frequency. | `COUNT(DISTINCT transaction_id)` |
| **total_foc_transactions**| The number of times they received free items. | How often they interact with our promotions. | `COUNT(DISTINCT transaction_id)` for FOC items |
| **median_sold_order_size**| The typical size of their paid orders. | Their buying pattern (big, rare orders vs. small, frequent ones). | `MEDIAN(sold_quantity)` per transaction |
| **median_foc_order_size**| The typical size of their free item orders. | The scale of promotions they usually engage with. | `MEDIAN(foc_quantity)` per transaction |

</details>

## 🏗️ System Architecture

The project is built upon a **Medallion Architecture**, a sequential data processing model that ensures quality, reliability, and scalability.

```
📊 RAW LAYER (Bronze) → 🧹 STAGING LAYER (Silver) → ⚙️ INTERMEDIATE LAYER (Gold) → 📈 MARTS LAYER (Platinum)
```

-   **🥉 Raw Layer (Bronze):** Acts as an immutable landing zone. Data is ingested from source systems in its original, untouched format, preserving a complete and faithful historical record.
-   **🥈 Staging Layer (Silver):** Focuses on data cleansing and standardization. It applies basic transformations, data type casting, and quality checks in a 1:1 mapping from the raw layer, without complex joins.
-   **🥇 Intermediate Layer (Gold):** This layer contains reusable, refined business logic. It joins data from different sources and creates aggregated features that serve as a "single source of truth" for business concepts.
-   **💎 Marts Layer (Platinum):** The final layer, delivering tailored "data products" for specific end-users. It contains optimized, aggregated tables (facts and dimensions) ready for BI tools, analytics, and as input for ML models.

## ⛓️ End-To-End Pipeline

This section outlines the complete data flow, from source to final data products, detailing the technology used at each stage.

1.  **Data Ingestion (Python & Pandas)**
    -   Raw data is extracted from source systems (SQLite files) and loaded into the initial `RAW` layer of our data warehouse.

2.  **Data Validation (Great Expectations)**
    -   Upon ingestion, data quality rules are executed to ensure that incoming data meets our integrity and format standards before any transformation.

3.  **Warehouse Engine (dbt & DuckDB)**
    -   **Storage:** DuckDB serves as our analytical database, holding all data layers (Raw, Staging, Intermediate, Marts).
    -   **Transformation:** dbt executes a series of SQL models that clean, standardize, and aggregate the data through the Medallion architecture layers. All business logic resides here.

4.  **Orchestration & CI/CD (Airflow & Scripts)**
    -   **Orchestration:** Apache Airflow coordinates the execution of the entire pipeline, scheduling and triggering ingestion and dbt transformation tasks in the correct sequence.
    -   **CI/CD:** Automated scripts (e.g., via `Makefile`) are used to run tests and validate the dbt models, ensuring code quality before deployment.

5.  **Data Products & Outputs**
    -   **ML Model (scikit-learn):** The transformed data from the `MARTS` layer is used to train a k-means clustering model for customer segmentation.
    -   **Self-Service Datasets:** The final `MARTS` tables are made available as clean, aggregated datasets for BI tools and business analysis.
    -   **Data Lineage (dbt docs):** dbt automatically generates a complete lineage graph, documenting how data flows and transforms from source to final model.

## 📁 Project Structure

```
werfen-data-impact-case/
├── 📊 data/                          # Source and processed data
│   ├── raw/                          # Raw data files (SQLite)
│   ├── processed/                    # Intermediate processing
│   └── training/                     # ML training datasets
├── 🔧 dbt_project/                   # dbt transformations
│   ├── models/                       # Data models (staging/intermediate/marts)
│   ├── macros/                       # Reusable SQL macros
│   ├── tests/                        # Data quality tests
│   └── docs/                         # Generated documentation
├── 📓 notebooks/                     # Analysis and demos
│   └── 1_End-To-End-Solution_Fixed.ipynb
├── 🏗️ src/                          # Main source code
│   ├── analysis/                     # Data Warehouse analysis tools
│   ├── ingestion/                    # Data ingestion pipeline
│   ├── validation/                   # Great Expectations setup
│   ├── ml_pipeline/                  # Machine Learning components
│   │   ├── clustering.py             # K-means implementation
│   │   ├── feature_engineering.py   # Feature creation & selection
│   │   ├── persona_assignment.py    # Business logic for personas
│   │   └── pipeline.py               # ML orchestration
│   ├── logging/                      # Structured logging system
│   ├── security/                     # Credential management
│   ├── performance/                  # DuckDB optimization & caching
│   ├── distributed/                  # Distributed system components
│   ├── portability/                  # Multi-cloud adapters
│   └── utils/                        # Utilities and validations
├── 📋 docs/                          # Technical documentation (Spanish)
├── ⚙️ config.py                      # Unified configuration management
├── 🔐 .credentials/                  # Encrypted credentials
├── 🚀 dags/                          # Apache Airflow DAGs
└── 🧪 tests/                         # Unit and integration tests
```

## 🚀 Quick Start

### 1. Environment Setup

```bash
# Clone repository
git clone <repository-url>
cd werfen-data-impact-case

# Create virtual environment
python -m venv venv_werfen
source venv_werfen/bin/activate  # Linux/Mac
# venv_werfen\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Setup environment
python setup_environment.py
```

### 2. Interactive Demo (Recommended)

**📓 [End-to-End Solution Notebook](notebooks/1_End-To-End-Solution.ipynb)**

This Jupyter notebook provides a complete interactive walkthrough of the entire data pipeline, from raw data ingestion to ML model training. It demonstrates:
- Data warehouse construction through Medallion architecture
- Quality validations with Great Expectations  
- Customer segmentation using K-means clustering
- Business insights and persona analysis

```bash
# Launch interactive demo
jupyter notebook notebooks/1_End-To-End-Solution.ipynb
```

### 3. Alternative Execution Methods

```bash
# Option 1: Direct script execution
python -m src.analysis.data_warehouse_analysis

# Option 2: ML Pipeline only
python -m src.ml_pipeline.run_ml_pipeline

# Option 3: Airflow orchestration
python -m dags.werfen_data_pipeline_dag
```

## 📞 Support & Contact

This project demonstrates advanced data engineering capabilities for Werfen's enterprise data needs.

---

**Developed by**: Daniel Moreno

**Project Type**: End-to-End Enterprise Data Architecture