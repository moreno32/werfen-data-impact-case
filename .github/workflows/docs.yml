name: 📚 Documentation

on:
  push:
    branches: [ main ]
    paths:
      - 'docs/**'
      - 'src/**'
      - 'README.md'
  pull_request:
    branches: [ main ]
    paths:
      - 'docs/**'
      - 'README.md'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================
  # JOB 1: SIMPLE DOCUMENTATION BUILD
  # ==========================================
  build-docs:
    name: 📖 Build Documentation
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install Documentation Tools
      run: |
        pip install -r requirements.txt
        
    - name: 📚 Generate Werfen Documentation
      run: |
        echo "📚 Generating Werfen Data Impact Case documentation..."
        
        # Create documentation structure
        mkdir -p docs/build docs/api docs/architecture docs/business
        
        # Generate project overview
        cat > docs/build/index.md << 'EOF'
        # 🎯 Werfen Data Impact Case - Documentation
        
        ## 📋 Project Overview
        
        This documentation covers the complete Werfen Data Impact Case solution, showcasing a modern enterprise data architecture with ML capabilities.
        
        ## 🏗️ Architecture Components
        
        - **Data Pipeline**: Medallion architecture (Bronze → Silver → Gold → Platinum)
        - **ML Pipeline**: Customer segmentation using K-means clustering
        - **Orchestration**: Apache Airflow for automated workflows
        - **Data Transformation**: dbt for data modeling and lineage
        - **Analytics**: Advanced customer persona analysis
        
        ## 🎯 Business Impact
        
        - Revenue optimization through targeted campaigns
        - Customer segmentation for personalized experiences  
        - Data-driven decision making capabilities
        - Scalable cloud-ready architecture
        
        ## 🚀 AWS Migration Ready
        
        The solution is designed for seamless AWS migration:
        - S3 for data lake storage
        - Redshift/DuckDB for data warehousing
        - SageMaker for ML operations
        - MWAA for Airflow orchestration
        
        EOF
        
        # Generate API documentation
        python -c "
        import inspect
        import importlib
        from pathlib import Path
        
        # Document key modules
        modules = [
            'src.analysis.data_warehouse_analysis',
            'src.ml_pipeline.pipeline',
            'src.portability.config_manager'
        ]
        
        api_doc = '# 📡 API Documentation\n\n'
        
        for module_name in modules:
            try:
                module = importlib.import_module(module_name)
                api_doc += f'## {module_name}\n\n'
                
                # Get classes and functions
                for name, obj in inspect.getmembers(module):
                    if inspect.isclass(obj) and obj.__module__ == module_name:
                        api_doc += f'### {name}\n'
                        if obj.__doc__:
                            api_doc += f'{obj.__doc__}\n\n'
                        else:
                            api_doc += 'No documentation available.\n\n'
                            
            except ImportError:
                api_doc += f'## {module_name}\n\nModule not found or import error.\n\n'
        
        with open('docs/api/api_reference.md', 'w') as f:
            f.write(api_doc)
        
        print('✅ API documentation generated')
        "
        
        # Generate architecture documentation
        cat > docs/architecture/system_design.md << 'EOF'
        # 🏗️ System Architecture
        
        ## Data Flow Architecture
        
        ```
        Raw Data → Bronze Layer → Silver Layer → Gold Layer → Platinum Layer
                                                                    ↓
                                                            ML Pipeline
                                                                    ↓
                                                          Customer Segments
        ```
        
        ## Component Overview
        
        ### Data Layers (Medallion Architecture)
        - **Bronze**: Raw data ingestion
        - **Silver**: Cleaned and validated data
        - **Gold**: Business-ready aggregated data
        - **Platinum**: ML-enhanced insights
        
        ### ML Pipeline
        - K-means clustering for customer segmentation
        - Feature engineering for business metrics
        - Persona assignment (Champions, Loyalists, Potential, At-Risk)
        
        ### Orchestration
        - Airflow DAGs for pipeline automation
        - Scheduled training and inference
        - Data quality monitoring
        
        EOF
        
        echo "✅ Werfen documentation generated successfully"
        
    - name: 📊 Documentation Summary
      run: |
        echo "📊 Documentation Build Summary:"
        echo "================================"
        echo "🎯 Project: Werfen Data Impact Case"
        echo "📅 Build Date: $(date -u)"
        echo "🔗 Commit: ${{ github.sha }}"
        echo ""
        echo "📚 Generated Documentation:"
        find docs/build -name "*.md" -exec basename {} \; | sed 's/^/  - /'
        find docs/api -name "*.md" -exec basename {} \; | sed 's/^/  - /'
        find docs/architecture -name "*.md" -exec basename {} \; | sed 's/^/  - /'
        echo ""
        echo "✅ Documentation ready for deployment"
        
    - name: 📊 Upload Documentation
      uses: actions/upload-artifact@v3
      with:
        name: werfen-documentation
        path: docs/ 