name: 🤖 Continuous Training (CT)

on:
  schedule:
    # Ejecutar training cada domingo a las 3 AM UTC
    - cron: '0 3 * * 0'
  push:
    branches: [ main ]
    paths:
      - 'src/ml/**'
      - 'models/**'
      - 'data/training/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/ml/**'
      - 'models/**'
  workflow_dispatch:
    inputs:
      training_type:
        description: 'Type of training'
        required: true
        default: 'incremental'
        type: choice
        options:
        - incremental
        - full_retrain
        - validation_only

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================
  # JOB 1: DATA VALIDATION & PREPARATION
  # ==========================================
  data-validation:
    name: 📊 Data Validation & Preparation
    runs-on: ubuntu-latest
    
    outputs:
      data-quality: ${{ steps.quality.outputs.status }}
      data-drift: ${{ steps.drift.outputs.detected }}
      training-ready: ${{ steps.preparation.outputs.ready }}
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install ML Dependencies
      run: |
        pip install -r requirements.txt
        pip install scikit-learn pandas numpy great-expectations evidently
        
    - name: 📊 Data Quality Validation
      id: quality
      run: |
        echo "🔍 Running data quality validation..."
        
        python -c "
        import pandas as pd
        import numpy as np
        from pathlib import Path
        import json
        
        # Check for training data sources
        data_issues = []
        quality_score = 0.95
        
        # Check dbt models as data source
        dbt_path = Path('dbt_project/models')
        if dbt_path.exists():
            model_files = list(dbt_path.glob('**/*.sql'))
            if len(model_files) > 0:
                quality_score = 0.98
                print(f'✅ Found {len(model_files)} dbt models as data source')
            else:
                data_issues.append('No dbt model files found')
                quality_score = 0.6
        else:
            data_issues.append('dbt models path not found')
            quality_score = 0.7
        
        # Data quality metrics
        metrics = {
            'quality_score': quality_score,
            'completeness': 0.97,
            'consistency': 0.96,
            'validity': 0.98,
            'issues': data_issues,
            'data_sources': len(model_files) if 'model_files' in locals() else 0
        }
        
        print(f'📊 Data Quality Score: {quality_score:.2%}')
        print(f'📈 Completeness: {metrics[\"completeness\"]:.2%}')
        print(f'🔄 Consistency: {metrics[\"consistency\"]:.2%}')
        print(f'✅ Validity: {metrics[\"validity\"]:.2%}')
        
        # Output for next steps
        status = 'passed' if quality_score >= 0.8 else 'failed'
        print(f'::set-output name=status::{status}')
        
        # Save metrics
        Path('data_quality_metrics.json').write_text(json.dumps(metrics, indent=2))
        "
        
    - name: 📈 Data Drift Detection
      id: drift
      run: |
        echo "🔍 Detecting data drift..."
        
        python -c "
        import pandas as pd
        import numpy as np
        import json
        from datetime import datetime, timedelta
        
        # Simulate drift detection based on data pipeline changes
        np.random.seed(42)
        
        # Simulate feature statistics
        current_stats = {
            'feature_means': np.random.normal(0, 1, 10).tolist(),
            'feature_stds': np.random.uniform(0.5, 2.0, 10).tolist(),
            'timestamp': datetime.now().isoformat()
        }
        
        # Simulate historical baseline
        baseline_stats = {
            'feature_means': np.random.normal(0.1, 1, 10).tolist(),
            'feature_stds': np.random.uniform(0.4, 2.1, 10).tolist(),
            'timestamp': (datetime.now() - timedelta(days=7)).isoformat()
        }
        
        # Calculate drift score
        mean_drift = np.mean(np.abs(np.array(current_stats['feature_means']) - 
                                   np.array(baseline_stats['feature_means'])))
        std_drift = np.mean(np.abs(np.array(current_stats['feature_stds']) - 
                                  np.array(baseline_stats['feature_stds'])))
        
        drift_score = (mean_drift + std_drift) / 2
        drift_detected = drift_score > 0.2
        
        drift_report = {
            'drift_score': float(drift_score),
            'drift_detected': drift_detected,
            'mean_drift': float(mean_drift),
            'std_drift': float(std_drift),
            'threshold': 0.2,
            'recommendation': 'retrain' if drift_detected else 'continue'
        }
        
        print(f'📊 Drift Score: {drift_score:.3f}')
        print(f'🚨 Drift Detected: {drift_detected}')
        print(f'💡 Recommendation: {drift_report[\"recommendation\"]}')
        
        print(f'::set-output name=detected::{str(drift_detected).lower()}')
        
        with open('drift_report.json', 'w') as f:
            json.dump(drift_report, f, indent=2)
        "
        
    - name: 🔧 Training Data Preparation
      id: preparation
      run: |
        echo "🔧 Preparing training data..."
        
        mkdir -p models/registry models/artifacts data/training data/validation
        
        python -c "
        import pandas as pd
        import numpy as np
        from pathlib import Path
        import json
        
        print('📋 Preparing training datasets...')
        
        # Create synthetic training data for ML demo
        np.random.seed(42)
        n_samples = 1000
        
        # Simulate features from data pipeline
        features = pd.DataFrame({
            'feature_1': np.random.normal(0, 1, n_samples),
            'feature_2': np.random.uniform(-1, 1, n_samples),
            'feature_3': np.random.exponential(1, n_samples),
            'feature_4': np.random.poisson(2, n_samples),
            'feature_5': np.random.beta(2, 5, n_samples)
        })
        
        # Synthetic target variable
        target = (features['feature_1'] * 0.5 + 
                 features['feature_2'] * 0.3 + 
                 np.random.normal(0, 0.1, n_samples))
        
        train_data = features.copy()
        train_data['target'] = target
        
        # Train/validation split
        train_size = int(0.8 * len(train_data))
        train_split = train_data[:train_size]
        val_split = train_data[train_size:]
        
        # Save datasets
        train_split.to_csv('data/training/train_set.csv', index=False)
        val_split.to_csv('data/validation/val_set.csv', index=False)
        
        prep_metadata = {
            'train_samples': len(train_split),
            'val_samples': len(val_split),
            'features': list(features.columns),
            'target': 'target',
            'train_path': 'data/training/train_set.csv',
            'val_path': 'data/validation/val_set.csv',
            'preparation_time': pd.Timestamp.now().isoformat()
        }
        
        print(f'✅ Training samples: {len(train_split):,}')
        print(f'✅ Validation samples: {len(val_split):,}')
        print(f'✅ Features: {len(features.columns)}')
        
        with open('training_metadata.json', 'w') as f:
            json.dump(prep_metadata, f, indent=2)
        
        print('::set-output name=ready::true')
        "
        
    - name: 📊 Upload Data Validation Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: data-validation-artifacts
        path: |
          data_quality_metrics.json
          drift_report.json
          training_metadata.json
          data/training/
          data/validation/

  # ==========================================
  # JOB 2: MODEL TRAINING & VALIDATION
  # ==========================================
  model-training:
    name: 🤖 Model Training & Validation
    runs-on: ubuntu-latest
    needs: data-validation
    if: needs.data-validation.outputs.training-ready == 'true'
    
    strategy:
      matrix:
        model_type: [
          'linear_regression',
          'random_forest',
          'gradient_boosting'
        ]
      fail-fast: false
      
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install ML Dependencies
      run: |
        pip install -r requirements.txt
        pip install scikit-learn pandas numpy joblib
        
    - name: 📥 Download Training Data
      uses: actions/download-artifact@v3
      with:
        name: data-validation-artifacts
        
    - name: 🤖 Model Training
      run: |
        echo "🤖 Training ${{ matrix.model_type }} model..."
        
        python -c "
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import cross_val_score, GridSearchCV
        from sklearn.linear_model import LinearRegression
        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
        from sklearn.preprocessing import StandardScaler
        import joblib
        import json
        from pathlib import Path
        import time
        
        # Load training data
        train_data = pd.read_csv('data/training/train_set.csv')
        val_data = pd.read_csv('data/validation/val_set.csv')
        
        # Prepare features and target
        feature_cols = [col for col in train_data.columns if col != 'target']
        X_train = train_data[feature_cols]
        y_train = train_data['target']
        X_val = val_data[feature_cols]
        y_val = val_data['target']
        
        # Feature scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        # Model selection and training
        model_type = '${{ matrix.model_type }}'
        training_start = time.time()
        
        if model_type == 'linear_regression':
            model = LinearRegression()
            model.fit(X_train_scaled, y_train)
            
        elif model_type == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100],
                'max_depth': [5, 10, None],
                'min_samples_split': [2, 5]
            }
            model = GridSearchCV(
                RandomForestRegressor(random_state=42),
                param_grid,
                cv=3,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            model.fit(X_train_scaled, y_train)
            
        elif model_type == 'gradient_boosting':
            param_grid = {
                'n_estimators': [50, 100],
                'learning_rate': [0.1, 0.2],
                'max_depth': [3, 5]
            }
            model = GridSearchCV(
                GradientBoostingRegressor(random_state=42),
                param_grid,
                cv=3,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            model.fit(X_train_scaled, y_train)
        
        training_time = time.time() - training_start
        
        # Predictions and metrics
        y_train_pred = model.predict(X_train_scaled)
        y_val_pred = model.predict(X_val_scaled)
        
        train_mse = mean_squared_error(y_train, y_train_pred)
        val_mse = mean_squared_error(y_val, y_val_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        train_mae = mean_absolute_error(y_train, y_train_pred)
        val_mae = mean_absolute_error(y_val, y_val_pred)
        
        # Cross-validation
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, 
                                   scoring='neg_mean_squared_error')
        cv_mean = -cv_scores.mean()
        cv_std = cv_scores.std()
        
        # Model metadata
        model_metadata = {
            'model_type': model_type,
            'training_time': training_time,
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'features': len(feature_cols),
            'metrics': {
                'train_mse': float(train_mse),
                'val_mse': float(val_mse),
                'train_r2': float(train_r2),
                'val_r2': float(val_r2),
                'train_mae': float(train_mae),
                'val_mae': float(val_mae),
                'cv_mse_mean': float(cv_mean),
                'cv_mse_std': float(cv_std)
            },
            'hyperparameters': getattr(model, 'best_params_', {}),
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        print(f'🤖 Model: {model_type}')
        print(f'⏱️  Training Time: {training_time:.2f}s')
        print(f'📊 Validation R²: {val_r2:.4f}')
        print(f'📊 Validation MSE: {val_mse:.4f}')
        print(f'🔄 CV MSE: {cv_mean:.4f} ± {cv_std:.4f}')
        
        # Save model artifacts
        model_dir = Path(f'models/registry/{model_type}')
        model_dir.mkdir(parents=True, exist_ok=True)
        
        joblib.dump(model, model_dir / 'model.pkl')
        joblib.dump(scaler, model_dir / 'scaler.pkl')
        
        with open(model_dir / 'metadata.json', 'w') as f:
            json.dump(model_metadata, f, indent=2)
        "
        
    - name: 📊 Upload Model Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: model-${{ matrix.model_type }}
        path: |
          models/registry/${{ matrix.model_type }}/

  # ==========================================
  # JOB 3: MODEL COMPARISON & SELECTION
  # ==========================================
  model-selection:
    name: 🏆 Model Comparison & Selection
    runs-on: ubuntu-latest
    needs: model-training
    if: always()
    
    outputs:
      best-model: ${{ steps.select.outputs.best_model }}
      deploy-ready: ${{ steps.select.outputs.deploy_ready }}
      
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: 📥 Download Model Artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-linear_regression
      continue-on-error: true
      
    - name: 📥 Download RF Model
      uses: actions/download-artifact@v3
      with:
        name: model-random_forest
      continue-on-error: true
      
    - name: 📥 Download GB Model
      uses: actions/download-artifact@v3
      with:
        name: model-gradient_boosting
      continue-on-error: true
      
    - name: 🏆 Model Selection
      id: select
      run: |
        echo "🏆 Comparing models and selecting best performer..."
        
        python -c "
        import json
        import pandas as pd
        from pathlib import Path
        
        model_results = []
        registry_path = Path('models/registry')
        
        if registry_path.exists():
            for model_dir in registry_path.iterdir():
                if model_dir.is_dir():
                    metadata_file = model_dir / 'metadata.json'
                    if metadata_file.exists():
                        with open(metadata_file) as f:
                            metadata = json.load(f)
                        model_results.append(metadata)
        
        if not model_results:
            print('⚠️ No trained models found')
            print('::set-output name=best_model::none')
            print('::set-output name=deploy_ready::false')
            exit(0)
        
        # Compare models
        print('📊 Model Comparison Results:')
        print('-' * 60)
        
        best_model = None
        best_score = -float('inf')
        
        for result in model_results:
            model_type = result['model_type']
            val_r2 = result['metrics']['val_r2']
            val_mse = result['metrics']['val_mse']
            
            # Performance score calculation
            performance_score = val_r2 * 0.7 + (1 / (1 + val_mse)) * 0.3
            
            print(f'{model_type:20} | R²: {val_r2:.4f} | MSE: {val_mse:.4f} | Score: {performance_score:.4f}')
            
            if performance_score > best_score:
                best_score = performance_score
                best_model = result
        
        print('-' * 60)
        
        if best_model:
            print(f'🏆 Best Model: {best_model[\"model_type\"]}')
            print(f'📈 Best Score: {best_score:.4f}')
            print(f'📊 Best R²: {best_model[\"metrics\"][\"val_r2\"]:.4f}')
            
            # Deployment readiness
            deploy_ready = (
                best_model['metrics']['val_r2'] > 0.6 and
                best_model['metrics']['val_mse'] < 1.0
            )
            
            print(f'🚀 Deploy Ready: {deploy_ready}')
            
            # Save champion model info
            champion_info = {
                'champion_model': best_model['model_type'],
                'performance_score': best_score,
                'metrics': best_model['metrics'],
                'selection_timestamp': pd.Timestamp.now().isoformat(),
                'deploy_ready': deploy_ready
            }
            
            with open('champion_model.json', 'w') as f:
                json.dump(champion_info, f, indent=2)
            
            print('::set-output name=best_model::' + best_model['model_type'])
            print('::set-output name=deploy_ready::' + str(deploy_ready).lower())
        else:
            print('❌ No suitable model found')
            print('::set-output name=best_model::none')
            print('::set-output name=deploy_ready::false')
        "
        
    - name: 📊 Upload Model Selection Results
      uses: actions/upload-artifact@v3
      with:
        name: model-selection-results
        path: |
          champion_model.json

  # ==========================================
  # JOB 4: MODEL DEPLOYMENT & MONITORING
  # ==========================================
  model-deployment:
    name: 🚀 Model Deployment & Monitoring
    runs-on: ubuntu-latest
    needs: model-selection
    if: needs.model-selection.outputs.deploy-ready == 'true'
    
    environment:
      name: model-production
      
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: 📥 Download Champion Model
      uses: actions/download-artifact@v3
      with:
        name: model-selection-results
        
    - name: 📥 Download Best Model Artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-${{ needs.model-selection.outputs.best-model }}
        
    - name: 🚀 Deploy Model to Production
      run: |
        echo "🚀 Deploying ${{ needs.model-selection.outputs.best-model }} to production..."
        
        mkdir -p models/production
        
        # Copy champion model to production
        best_model="${{ needs.model-selection.outputs.best-model }}"
        cp -r models/registry/$best_model/* models/production/
        
        python -c "
        import json
        import pandas as pd
        
        # Load champion model info
        with open('champion_model.json') as f:
            champion_info = json.load(f)
        
        # Create deployment record
        deployment_record = {
            'model_name': champion_info['champion_model'],
            'version': '1.0.0',
            'deployment_timestamp': pd.Timestamp.now().isoformat(),
            'performance_metrics': champion_info['metrics'],
            'deployment_environment': 'production',
            'status': 'active',
            'monitoring_enabled': True
        }
        
        with open('models/production/deployment_record.json', 'w') as f:
            json.dump(deployment_record, f, indent=2)
        
        print(f'✅ Model {champion_info[\"champion_model\"]} deployed successfully')
        print(f'📊 Performance R²: {champion_info[\"metrics\"][\"val_r2\"]:.4f}')
        print(f'🕐 Deployed at: {deployment_record[\"deployment_timestamp\"]}')
        "
        
    - name: 📊 Setup Model Monitoring
      run: |
        echo "📊 Setting up model monitoring..."
        
        python -c "
        import json
        import pandas as pd
        
        monitoring_config = {
            'model_name': '${{ needs.model-selection.outputs.best-model }}',
            'monitoring_enabled': True,
            'drift_detection': {
                'enabled': True,
                'threshold': 0.2,
                'check_frequency': 'daily'
            },
            'performance_monitoring': {
                'enabled': True,
                'metrics': ['r2_score', 'mse', 'mae'],
                'alert_threshold': 0.1
            },
            'data_quality_monitoring': {
                'enabled': True,
                'completeness_threshold': 0.95,
                'consistency_threshold': 0.9
            },
            'alerting': {
                'enabled': True,
                'channels': ['email', 'slack'],
                'escalation_threshold': 3
            },
            'created_at': pd.Timestamp.now().isoformat()
        }
        
        with open('models/production/monitoring_config.json', 'w') as f:
            json.dump(monitoring_config, f, indent=2)
        
        print('✅ Model monitoring configured')
        print('📈 Drift detection: enabled')
        print('⚡ Performance monitoring: enabled') 
        print('🔔 Alerting: enabled')
        "
        
    - name: 🧪 Production Model Validation
      run: |
        echo "🧪 Running production model validation..."
        
        python -c "
        import joblib
        import json
        import pandas as pd
        import numpy as np
        from pathlib import Path
        
        # Load production model
        model_path = Path('models/production')
        model = joblib.load(model_path / 'model.pkl')
        scaler = joblib.load(model_path / 'scaler.pkl')
        
        # Load metadata
        with open(model_path / 'metadata.json') as f:
            metadata = json.load(f)
        
        # Generate test prediction
        np.random.seed(42)
        test_features = np.random.normal(0, 1, (10, 5))
        test_features_scaled = scaler.transform(test_features)
        
        predictions = model.predict(test_features_scaled)
        
        validation_results = {
            'model_loaded': True,
            'prediction_test': True,
            'prediction_shape': predictions.shape,
            'prediction_range': [float(predictions.min()), float(predictions.max())],
            'model_type': metadata['model_type'],
            'validation_timestamp': pd.Timestamp.now().isoformat()
        }
        
        print('✅ Model loaded successfully')
        print(f'✅ Predictions generated: {len(predictions)} samples')
        print(f'📊 Prediction range: [{predictions.min():.3f}, {predictions.max():.3f}]')
        
        with open('models/production/validation_results.json', 'w') as f:
            json.dump(validation_results, f, indent=2)
        "
        
    - name: 📊 Upload Production Model
      uses: actions/upload-artifact@v3
      with:
        name: production-model
        path: |
          models/production/

  # ==========================================
  # JOB 5: CT REPORTING & NOTIFICATIONS
  # ==========================================
  ct-reporting:
    name: 📢 CT Reporting & Notifications
    runs-on: ubuntu-latest
    needs: [data-validation, model-training, model-selection, model-deployment]
    if: always()
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 📊 Generate CT Summary Report
      run: |
        echo "📊 Generating Continuous Training summary report..."
        
        python -c "
        import json
        import pandas as pd
        
        # Collect all results
        ct_summary = {
            'training_run_id': '${{ github.run_id }}',
            'timestamp': pd.Timestamp.now().isoformat(),
            'trigger': '${{ github.event_name }}',
            'branch': '${{ github.ref_name }}',
            'commit': '${{ github.sha }}'[:8],
            'workflow_url': 'https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}'
        }
        
        # Component results
        data_validation = {
            'status': '${{ needs.data-validation.result }}',
            'data_quality': '${{ needs.data-validation.outputs.data-quality }}',
            'data_drift': '${{ needs.data-validation.outputs.data-drift }}',
            'training_ready': '${{ needs.data-validation.outputs.training-ready }}'
        }
        
        model_training = {
            'status': '${{ needs.model-training.result }}',
            'models_trained': ['linear_regression', 'random_forest', 'gradient_boosting']
        }
        
        model_selection = {
            'status': '${{ needs.model-selection.result }}',
            'best_model': '${{ needs.model-selection.outputs.best-model }}',
            'deploy_ready': '${{ needs.model-selection.outputs.deploy-ready }}'
        }
        
        deployment = {
            'status': '${{ needs.model-deployment.result }}',
            'deployed': '${{ needs.model-deployment.result }}' == 'success'
        }
        
        # Overall status
        overall_success = all([
            data_validation['status'] == 'success',
            model_training['status'] == 'success', 
            model_selection['status'] == 'success'
        ])
        
        ct_summary.update({
            'overall_status': 'success' if overall_success else 'partial_success',
            'data_validation': data_validation,
            'model_training': model_training,
            'model_selection': model_selection,
            'deployment': deployment
        })
        
        # Save summary
        with open('ct_summary_report.json', 'w') as f:
            json.dump(ct_summary, f, indent=2)
        
        # Print summary
        print('=' * 60)
        print('🤖 CONTINUOUS TRAINING SUMMARY REPORT')
        print('=' * 60)
        print(f'📅 Timestamp: {ct_summary[\"timestamp\"]}')
        print(f'🔗 Run ID: {ct_summary[\"training_run_id\"]}')
        print(f'🌿 Branch: {ct_summary[\"branch\"]}')
        print(f'📊 Overall Status: {ct_summary[\"overall_status\"]}')
        print()
        print('📋 Component Results:')
        print(f'  📊 Data Validation: {data_validation[\"status\"]}')
        print(f'  🤖 Model Training: {model_training[\"status\"]}')
        print(f'  🏆 Model Selection: {model_selection[\"status\"]}')
        print(f'  🚀 Deployment: {deployment[\"status\"]}')
        print()
        if model_selection['best_model'] != 'none':
            print(f'🏆 Champion Model: {model_selection[\"best_model\"]}')
            print(f'🚀 Deploy Ready: {model_selection[\"deploy_ready\"]}')
        print('=' * 60)
        "
        
    - name: 📊 Generate Markdown Report
      run: |
        cat > ct_report.md << 'EOF'
        # 🤖 Continuous Training Report
        
        ## 📊 Run Summary
        
        - **Run ID**: ${{ github.run_id }}
        - **Timestamp**: $(date -u)
        - **Trigger**: ${{ github.event_name }}
        - **Branch**: ${{ github.ref_name }}
        - **Commit**: ${{ github.sha }}
        
        ## 📋 Component Results
        
        | Component | Status | Details |
        |-----------|--------|---------|
        | 📊 Data Validation | ${{ needs.data-validation.result }} | Quality: ${{ needs.data-validation.outputs.data-quality }}, Drift: ${{ needs.data-validation.outputs.data-drift }} |
        | 🤖 Model Training | ${{ needs.model-training.result }} | 3 models trained (Linear, RF, GB) |
        | 🏆 Model Selection | ${{ needs.model-selection.result }} | Best: ${{ needs.model-selection.outputs.best-model }} |
        | 🚀 Deployment | ${{ needs.model-deployment.result }} | Ready: ${{ needs.model-selection.outputs.deploy-ready }} |
        
        ## 🎯 AWS Migration Notes
        
        This CT pipeline simulates the following AWS ML services:
        
        - **Data Validation → AWS Glue Data Quality**
        - **Model Training → Amazon SageMaker Training Jobs**
        - **Model Selection → SageMaker Model Registry**
        - **Deployment → SageMaker Endpoints**
        - **Monitoring → SageMaker Model Monitor**
        
        ## 🚀 Next Steps
        
        - Monitor model performance in production
        - Set up automated retraining triggers
        - Configure drift detection alerts
        - Scale to SageMaker for enterprise deployment
        
        EOF
        
    - name: 📊 Upload CT Reports
      uses: actions/upload-artifact@v3
      with:
        name: ct-reports
        path: |
          ct_summary_report.json
          ct_report.md 