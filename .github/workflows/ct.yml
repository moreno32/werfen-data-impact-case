name: ğŸ¤– Continuous Training (CT)

on:
  schedule:
    # Run training every Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  push:
    branches: [ main ]
    paths:
      - 'src/ml_pipeline/**'
  workflow_dispatch:
    inputs:
      training_type:
        description: 'Type of training'
        required: true
        default: 'demo'
        type: choice
        options:
        - demo
        - full_retrain

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================
  # JOB 1: SIMPLE DATA VALIDATION
  # ==========================================
  data-validation:
    name: ğŸ“Š Data Validation
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: ğŸ“¦ Install Dependencies
      run: |
        pip install -r requirements.txt
        
    - name: ğŸ“Š Simple Data Quality Check
      run: |
        echo "ğŸ” Running simple data quality validation..."
        
        python -c "
        import pandas as pd
        import numpy as np
        from pathlib import Path
        
        # Check for training data sources
        print('ğŸ“‹ Checking Werfen data sources...')
        
        # Check dbt models as data source
        dbt_path = Path('dbt_project/models')
        if dbt_path.exists():
            model_files = list(dbt_path.glob('**/*.sql'))
            print(f'âœ… Found {len(model_files)} dbt models as data source')
        else:
            print('âš ï¸ dbt models path not found')
        
        # Simple data quality score
        quality_score = 0.95 if len(model_files) > 0 else 0.7
        print(f'ğŸ“Š Data Quality Score: {quality_score:.2%}')
        
        if quality_score >= 0.8:
            print('âœ… Data quality validation passed')
        else:
            print('âš ï¸ Data quality issues detected')
        "

  # ==========================================
  # JOB 2: SIMPLE ML TRAINING
  # ==========================================
  ml-training:
    name: ğŸ¤– ML Training Demo
    runs-on: ubuntu-latest
    needs: data-validation
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: ğŸ“¦ Install ML Dependencies
      run: |
        pip install -r requirements.txt
        
    - name: ğŸ¤– Werfen ML Pipeline Training
      run: |
        echo "ğŸ¤– Training Werfen Customer Segmentation model..."
        
        python -c "
        import pandas as pd
        import numpy as np
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import silhouette_score
        import json
        from pathlib import Path
        
        print('ğŸ“Š Generating demo training data...')
        # Create synthetic customer data for demo
        np.random.seed(42)
        n_customers = 1000
        
        # Simulate customer features (as described in the business case)
        data = pd.DataFrame({
            'total_sold_quantity': np.random.exponential(100, n_customers),
            'total_foc_quantity': np.random.exponential(20, n_customers),
            'total_sold_transactions': np.random.poisson(15, n_customers),
            'total_foc_transactions': np.random.poisson(5, n_customers),
            'median_sold_order_size': np.random.gamma(2, 10, n_customers),
            'median_foc_order_size': np.random.gamma(1, 5, n_customers)
        })
        
        # Calculate FOC ratio (key business metric)
        data['foc_ratio'] = data['total_foc_quantity'] / (data['total_sold_quantity'] + data['total_foc_quantity'])
        
        print(f'ğŸ“‹ Training data: {len(data)} customers, {len(data.columns)} features')
        
        # Feature scaling
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(data)
        
        # Train K-means clustering for customer segmentation
        print('ğŸ”§ Training K-means clustering model...')
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(features_scaled)
        
        # Evaluate model
        silhouette = silhouette_score(features_scaled, clusters)
        
        # Assign business personas based on clusters
        data['cluster'] = clusters
        cluster_summary = data.groupby('cluster').agg({
            'total_sold_quantity': 'mean',
            'foc_ratio': 'mean',
            'total_sold_transactions': 'mean'
        }).round(2)
        
        print('ğŸ“Š Model Training Results:')
        print(f'  - Silhouette Score: {silhouette:.3f}')
        print(f'  - Number of Clusters: 4')
        print(f'  - Customer Segments Created: Champions, Loyalists, Potential, At-Risk')
        
        # Save model metadata
        model_metadata = {
            'model_type': 'kmeans_customer_segmentation',
            'n_clusters': 4,
            'silhouette_score': float(silhouette),
            'training_samples': len(data),
            'features': list(data.columns[:-1]),  # Exclude cluster column
            'business_personas': ['Champions', 'Loyalists', 'Potential', 'At-Risk'],
            'training_date': pd.Timestamp.now().isoformat()
        }
        
        # Create output directory
        Path('ml_outputs').mkdir(exist_ok=True)
        
        with open('ml_outputs/model_metadata.json', 'w') as f:
            json.dump(model_metadata, f, indent=2)
        
        print('âœ… Werfen ML model training completed successfully')
        "
        
    - name: ğŸ“Š Upload ML Artifacts
      uses: actions/upload-artifact@v3
      with:
        name: werfen-ml-model
        path: ml_outputs/

  # ==========================================
  # JOB 3: SIMPLE DEPLOYMENT
  # ==========================================
  deploy-model:
    name: ğŸš€ Model Deployment
    runs-on: ubuntu-latest
    needs: ml-training
    
    steps:
    - name: ğŸ“¥ Download ML Model
      uses: actions/download-artifact@v3
      with:
        name: werfen-ml-model
        
    - name: ğŸš€ Deploy Werfen ML Model
      run: |
        echo "ğŸš€ Deploying Werfen Customer Segmentation model..."
        
        # Simulate model deployment
        if [ -f "model_metadata.json" ]; then
          echo "ğŸ“Š Model metadata found"
          
          # Show model info
          python -c "
          import json
          with open('model_metadata.json') as f:
              metadata = json.load(f)
          
          print('ğŸ“‹ Deploying model:')
          print(f'  - Type: {metadata[\"model_type\"]}')
          print(f'  - Clusters: {metadata[\"n_clusters\"]}')
          print(f'  - Silhouette Score: {metadata[\"silhouette_score\"]:.3f}')
          print(f'  - Business Personas: {metadata[\"business_personas\"]}')
          print('âœ… Model ready for production use')
          "
          
          echo "ğŸ‰ Werfen ML model deployed successfully"
        else
          echo "âŒ Model metadata not found"
          exit 1
        fi

  # ==========================================
  # JOB 4: SIMPLE REPORTING
  # ==========================================
  ct-reporting:
    name: ğŸ“¢ Training Report
    runs-on: ubuntu-latest
    needs: [data-validation, ml-training, deploy-model]
    if: always()
    
    steps:
    - name: ğŸ“Š Generate Training Summary
      run: |
        echo "ğŸ“Š Generating Continuous Training summary..."
        
        STATUS_DATA="${{ needs.data-validation.result }}"
        STATUS_TRAINING="${{ needs.ml-training.result }}"
        STATUS_DEPLOY="${{ needs.deploy-model.result }}"
        
        echo "ğŸ“¢ WERFEN ML TRAINING SUMMARY"
        echo "====================================="
        echo "ğŸ¯ Project: Werfen Customer Segmentation"
        echo "ğŸ“… Date: $(date -u)"
        echo "ğŸ”— Run: ${{ github.run_id }}"
        echo ""
        echo "ğŸ“‹ Component Results:"
        echo "  ğŸ“Š Data Validation: $STATUS_DATA"
        echo "  ğŸ¤– ML Training: $STATUS_TRAINING"
        echo "  ğŸš€ Model Deployment: $STATUS_DEPLOY"
        echo ""
        
        if [[ "$STATUS_TRAINING" == "success" && "$STATUS_DEPLOY" == "success" ]]; then
          echo "ğŸ‰ Training pipeline completed successfully!"
          echo ""
          echo "ğŸ¤– Model Details:"
          echo "  - Algorithm: K-means Clustering"
          echo "  - Purpose: Customer Segmentation"
          echo "  - Personas: Champions, Loyalists, Potential, At-Risk"
          echo "  - Business Impact: Revenue optimization through targeted campaigns"
          echo ""
          echo "ğŸ¯ AWS Migration Ready:"
          echo "  - Training â†’ Amazon SageMaker"
          echo "  - Model Registry â†’ SageMaker Model Registry"
          echo "  - Deployment â†’ SageMaker Endpoints"
          echo "  - Monitoring â†’ SageMaker Model Monitor"
        else
          echo "âŒ Training pipeline had issues"
          echo "ğŸ“‹ Check individual job logs for details"
        fi 