name: 🤖 Continuous Training (CT)

on:
  schedule:
    # Run training every Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  push:
    branches: [ main ]
    paths:
      - 'src/ml_pipeline/**'
  workflow_dispatch:
    inputs:
      training_type:
        description: 'Type of training'
        required: true
        default: 'demo'
        type: choice
        options:
        - demo
        - full_retrain

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================
  # JOB 1: SIMPLE DATA VALIDATION
  # ==========================================
  data-validation:
    name: 📊 Data Validation
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        pip install -r requirements.txt
        
    - name: 📊 Simple Data Quality Check
      run: |
        echo "🔍 Running simple data quality validation..."
        
        python -c "
        import pandas as pd
        import numpy as np
        from pathlib import Path
        
        # Check for training data sources
        print('📋 Checking Werfen data sources...')
        
        # Check dbt models as data source
        dbt_path = Path('dbt_project/models')
        if dbt_path.exists():
            model_files = list(dbt_path.glob('**/*.sql'))
            print(f'✅ Found {len(model_files)} dbt models as data source')
        else:
            print('⚠️ dbt models path not found')
        
        # Simple data quality score
        quality_score = 0.95 if len(model_files) > 0 else 0.7
        print(f'📊 Data Quality Score: {quality_score:.2%}')
        
        if quality_score >= 0.8:
            print('✅ Data quality validation passed')
        else:
            print('⚠️ Data quality issues detected')
        "

  # ==========================================
  # JOB 2: SIMPLE ML TRAINING
  # ==========================================
  ml-training:
    name: 🤖 ML Training Demo
    runs-on: ubuntu-latest
    needs: data-validation
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install ML Dependencies
      run: |
        pip install -r requirements.txt
        
    - name: 🤖 Werfen ML Pipeline Training
      run: |
        echo "🤖 Training Werfen Customer Segmentation model..."
        
        python -c "
        import pandas as pd
        import numpy as np
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import silhouette_score
        import json
        from pathlib import Path
        
        print('📊 Generating demo training data...')
        # Create synthetic customer data for demo
        np.random.seed(42)
        n_customers = 1000
        
        # Simulate customer features (as described in the business case)
        data = pd.DataFrame({
            'total_sold_quantity': np.random.exponential(100, n_customers),
            'total_foc_quantity': np.random.exponential(20, n_customers),
            'total_sold_transactions': np.random.poisson(15, n_customers),
            'total_foc_transactions': np.random.poisson(5, n_customers),
            'median_sold_order_size': np.random.gamma(2, 10, n_customers),
            'median_foc_order_size': np.random.gamma(1, 5, n_customers)
        })
        
        # Calculate FOC ratio (key business metric)
        data['foc_ratio'] = data['total_foc_quantity'] / (data['total_sold_quantity'] + data['total_foc_quantity'])
        
        print(f'📋 Training data: {len(data)} customers, {len(data.columns)} features')
        
        # Feature scaling
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(data)
        
        # Train K-means clustering for customer segmentation
        print('🔧 Training K-means clustering model...')
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(features_scaled)
        
        # Evaluate model
        silhouette = silhouette_score(features_scaled, clusters)
        
        # Assign business personas based on clusters
        data['cluster'] = clusters
        cluster_summary = data.groupby('cluster').agg({
            'total_sold_quantity': 'mean',
            'foc_ratio': 'mean',
            'total_sold_transactions': 'mean'
        }).round(2)
        
        print('📊 Model Training Results:')
        print(f'  - Silhouette Score: {silhouette:.3f}')
        print(f'  - Number of Clusters: 4')
        print(f'  - Customer Segments Created: Champions, Loyalists, Potential, At-Risk')
        
        # Save model metadata
        model_metadata = {
            'model_type': 'kmeans_customer_segmentation',
            'n_clusters': 4,
            'silhouette_score': float(silhouette),
            'training_samples': len(data),
            'features': list(data.columns[:-1]),  # Exclude cluster column
            'business_personas': ['Champions', 'Loyalists', 'Potential', 'At-Risk'],
            'training_date': pd.Timestamp.now().isoformat()
        }
        
        # Create output directory
        Path('ml_outputs').mkdir(exist_ok=True)
        
        with open('ml_outputs/model_metadata.json', 'w') as f:
            json.dump(model_metadata, f, indent=2)
        
        print('✅ Werfen ML model training completed successfully')
        "
        
    - name: 📊 Upload ML Artifacts
      uses: actions/upload-artifact@v3
      with:
        name: werfen-ml-model
        path: ml_outputs/

  # ==========================================
  # JOB 3: SIMPLE DEPLOYMENT
  # ==========================================
  deploy-model:
    name: 🚀 Model Deployment
    runs-on: ubuntu-latest
    needs: ml-training
    
    steps:
    - name: 📥 Download ML Model
      uses: actions/download-artifact@v3
      with:
        name: werfen-ml-model
        
    - name: 🚀 Deploy Werfen ML Model
      run: |
        echo "🚀 Deploying Werfen Customer Segmentation model..."
        
        # Simulate model deployment
        if [ -f "model_metadata.json" ]; then
          echo "📊 Model metadata found"
          
          # Show model info
          python -c "
          import json
          with open('model_metadata.json') as f:
              metadata = json.load(f)
          
          print('📋 Deploying model:')
          print(f'  - Type: {metadata[\"model_type\"]}')
          print(f'  - Clusters: {metadata[\"n_clusters\"]}')
          print(f'  - Silhouette Score: {metadata[\"silhouette_score\"]:.3f}')
          print(f'  - Business Personas: {metadata[\"business_personas\"]}')
          print('✅ Model ready for production use')
          "
          
          echo "🎉 Werfen ML model deployed successfully"
        else
          echo "❌ Model metadata not found"
          exit 1
        fi

  # ==========================================
  # JOB 4: SIMPLE REPORTING
  # ==========================================
  ct-reporting:
    name: 📢 Training Report
    runs-on: ubuntu-latest
    needs: [data-validation, ml-training, deploy-model]
    if: always()
    
    steps:
    - name: 📊 Generate Training Summary
      run: |
        echo "📊 Generating Continuous Training summary..."
        
        STATUS_DATA="${{ needs.data-validation.result }}"
        STATUS_TRAINING="${{ needs.ml-training.result }}"
        STATUS_DEPLOY="${{ needs.deploy-model.result }}"
        
        echo "📢 WERFEN ML TRAINING SUMMARY"
        echo "====================================="
        echo "🎯 Project: Werfen Customer Segmentation"
        echo "📅 Date: $(date -u)"
        echo "🔗 Run: ${{ github.run_id }}"
        echo ""
        echo "📋 Component Results:"
        echo "  📊 Data Validation: $STATUS_DATA"
        echo "  🤖 ML Training: $STATUS_TRAINING"
        echo "  🚀 Model Deployment: $STATUS_DEPLOY"
        echo ""
        
        if [[ "$STATUS_TRAINING" == "success" && "$STATUS_DEPLOY" == "success" ]]; then
          echo "🎉 Training pipeline completed successfully!"
          echo ""
          echo "🤖 Model Details:"
          echo "  - Algorithm: K-means Clustering"
          echo "  - Purpose: Customer Segmentation"
          echo "  - Personas: Champions, Loyalists, Potential, At-Risk"
          echo "  - Business Impact: Revenue optimization through targeted campaigns"
          echo ""
          echo "🎯 AWS Migration Ready:"
          echo "  - Training → Amazon SageMaker"
          echo "  - Model Registry → SageMaker Model Registry"
          echo "  - Deployment → SageMaker Endpoints"
          echo "  - Monitoring → SageMaker Model Monitor"
        else
          echo "❌ Training pipeline had issues"
          echo "📋 Check individual job logs for details"
        fi 